\chapter{Conclusions and Future Work}
\label{chap:conclusion}

\section{Conclusions}

Thanks to the Information Extraction methods, a lot of facts in the format \textit{$<$subject$>$ $<$predicate$>$ $<$object$>$} are collected from the Internet. However, due to automatic construction, these facts can be incomplete. Rule mining approach such as AMIE system~\cite{ref10} is proposed to tackle this issue where the explored positive rules from the original KG can be exploited to predict new facts and extend KG.

With the aim to enhance the quality of positive rules and subsequently improve the accuracy of fact predictions, non-monotonic rule mining is taken into consideration in this thesis. This idea is proposed in~\cite{ref12} where KG is collection of unary facts. Meanwhile, RUMIS tool implemented in our research generates negative rules from KG in nature format.

More specifically, RUMIS is developed based on the theory framework where the novel concept of \textit{partial materialization} is introduced to revise positive Horn rules. Subsequently, the chosen revisions are exploited to extend the original data, and thus, tackle the KG completion problem. Some experiments in the thesis are conducted for testing quality of rules generated by RUMIS and the result supports the proposed theory.

\section{Future Directions}

In the future, there are some directions that we can develop as follows.

\begin{itemize}
\item \textbf{Tackle language bias challenge.} More forms for the positive rules can be implemented, not only the specific form introduced in Chapter~\ref{chap:system}. This makes the RUMIS less restrictive and diversify the resulting revisions. We can add more variables and unary predicates to the rule form, not only three variables and only binary predicates in the current system. Besides, the Horn rules that RUMIS mines now are translated from conjunctive queries, one possible future direction is to add existential operator to the body of the rule, e.g., \textit{isMarriedTo(X, Y) $\leftarrow$ studiesWith(X, Y): $\exists$ Z: isFriendOf(X, Z), isFriendOf(Y, Z)} can be taken into consideration.
\item \textbf{More form for the exceptions.} Currently RUMIS only supports one exception in the body and this exception is the relation between variables in the head. To extend the format of non-monotonic rules, we can increase number of exceptions in the body of the revision. E.g., some interesting rule such as \textit{isCitizenOf(X, Y) $\leftarrow$ bornIn(X, Y), not Manager(X), not Singapore(Y)} can be mined in the updated version of the system.
\item \textbf{Optimize database.} Due to large size of the KG, data indexing is a time burden step for every experiment. Thus, we can refine the RUMIS system by store the facts in a database and the rest computation may make us that via a web service subsequently. Thanks to this technique, we only build the service for querying data one time at the beginning, and the time for experiment is significantly reduced.
\item \textbf{Try more rule measures.} Other predictive measures and exception evaluation methods can be tested to search for interesting nonmonotonic rules. A survey in~\cite{ref46} specifies a variety of choices for predictive rule measure where conviction is only one of them.
\item \textbf{N-ary facts.} In YAGO and IMDB datasets, the facts are three-dimensional, i.e., each of them contains three parameters such as subject, predicate, object. The sampled Wikidata exploited in the experiment chapter has the same setting, i.e., it is extracted from simplified version of original Wikidata where all facts are projected to three-dimensional space. One possible future extension takes the n-ary facts, i.e., facts contain n parameters into consideration. E.g., the five-dimensional fact \textit{$<$Ronaldo$>$ $<$playsFor$>$ $<$Manchester United$>$ $<$in$>$ $<$2008$>$} may appear in original Wikidata.
\item \textbf{Sample training KG randomly.} In our current experiment, every learning KG is chosen from the ideal KG by removing 20\% facts for each binary predicate. This setting is restrictive because it tries to maintain the distribution of facts over the predicates. It is worth testing the quality of predictions generated by RUMIS if the training data is chosen randomly where the percentage of the retained facts are not the same for every relation.
\item \textbf{Optimize DLV.} DLV is the bottle neck in the current system. Indeed, with very large KG and 10 revised rules, it takes up to days to extend KG while RUMIS generates revisions in seconds or minutes. Since completing KG is the one of the main goals of the tool, DLV should be replaced by an upgraded and better version to make sure the run time performance is acceptable.
\item \textbf{Facts with probability.} For our experiment, the facts in the given KG are always true, this is not suitable in practice where some facts can be totally false. E.g., YAGO contains a lot of incorrect triples because part of this KG is collected automatically by information extraction approach. As a possible direction, assume that we know the probability of true facts in the training KG, some rule measure should take these numbers into consideration, resulting in new predicted facts with level of accuracy.
\end{itemize}