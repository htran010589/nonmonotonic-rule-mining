\chapter{Conclusions and Future Work}
\label{chap:conclusion}

\section{Conclusions}

Advances in Information Extraction have led to the construction of large KGs in the form of \textit{$\langle$subject predicate object$\rangle$} triples. However, due to the automatic construction, these KGs can be incomplete. Horn rule mining is a popular approach to address this issue~\cite{ref10}.

In this thesis we have looked at the problem of enhancing the quality of positive rules and subsequently improving the accuracy of their predictions, by revising the mined rules into the nonmonotonic ones. We follow the ideas from~\cite{ref12} where the same problem was studied for KGs containing only unary facts. Meanwhile, RUMIS tool implemented in our research generates nonmonotonic rules from KGs in their original format.

We have extended the results from~\cite{ref12} to KG revision in their original relational form and developed the RUMIS system, for learning nonmonotonic rules in KGs under the OWA. Subsequently, the chosen revisions are exploited to extend the original data, and thus, they are suitable for tackling the KG completion problem. Some experiments in the thesis are conducted for testing quality of rules generated by RUMIS and the results demonstrate that the proposed approach outperforms the state of the art KG rule learning systems w.r.t. to the quality of the made predictions.

\section{Future Directions}

We now discuss possible future directions.

\begin{itemize}
\item \textbf{Tackle language bias challenge.} In this work we have fixed the language bias of the rules to be mined. Certainly, a promising and natural direction is to extend our results to further rule forms and make the language bias more flexible. The language bias can be manually specified by users, which should make the RUMIS system less restrictive and diversify possible revisions. Another possible future direction is to consider existential operators in the heads of the positive rules, e.g., \textit{$\exists$ Y hasParent(X, Y) $\leftarrow$ person(X)}.
\item \textbf{Enriching exception forms.} Currently RUMIS only supports one exception in the body and this exception is the relation between variables in the head or a unary atom. To extend the form of nonmonotonic rules, we can increase the number of exceptions in the body of the revision. E.g., an interesting rule like \textit{isCitizenOf(X, Y) $\leftarrow$ bornIn(X, Y), not manager(X), not isAsiaCountry(Y)} could then be mined.
\item \textbf{Optimizations.} Due to the large size of the KGs, data indexing is a time burden step in our implementation. Thus, a natural optimization direction is to store the facts in a database and to build a web service for finding results of conjunctive relational queries. Thanks to this technique, we will only build the service once at the beginning, and the data indexing step will not be necessary for every execution of the RUMIS system. Hence, the total time of the experiment should be significantly reduced.
\item \textbf{Try more rule measures.} Other predictive measures and exception evaluation methods can be tested to search for interesting nonmonotonic rules. A survey in~\cite{ref46} specifies a variety of choices for predictive rule measures, where conviction is only one of them.
\item \textbf{N-ary facts.} In YAGO and IMDB datasets, the facts are three-dimensional, i.e., each of them contains a subject, a predicate and an object. The Wikidata Football dataset exploited in the experiments has the same property, i.e., it is extracted from simplified version of original Wikidata where all facts are projected to three-dimensional space. One possible future extension would be to take the n-ary facts, i.e., facts containing n-parameters, into consideration. E.g., the five-dimensional fact \textit{$<$Ronaldo$>$ $<$playsFor$>$ $<$Manchester United$>$ $<$in$>$ $<$2008$>$} may appear in the full Wikidata KG.
\item \textbf{Training KG random sampling.} In our current experiment, every learning KG is chosen from the ideal KG by removing 20\% of facts for each binary predicate. This setting is a bit restrictive, because it tries to maintain the distribution of facts over the predicates. It is worth testing the quality of predictions generated by RUMIS if the training data is chosen randomly by varying the percentage of the retained facts for every relation.
\item \textbf{Optimize DLV.} DLV running time is a burden in the current system. Indeed, for very large KGs and 10 revised rules, it takes up to days to produce predictions. Since completing a given KG is one of the main goals of the tool, optimizations of the DLV tool should be studied to make sure that the total run time performance is acceptable.
\item \textbf{Facts with probability.} For our experiment, the facts in the given KG are always true, which is not always the case, where some facts might be totally wrong. Indeed, modern KGs might contain incorrect triples, since their large parts are constructed automatically by information extraction techniques. As a possible future direction, probability can be assigned to facts in the training KG as the weight, and taken into account during the rule learning and revision. This will result in new predictions with confidence weights assigned to them.
\end{itemize}