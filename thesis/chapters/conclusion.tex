\chapter{Conclusions and Future Work}
\label{chap:conclusion}

\section{Conclusions}

Advances in Information Extraction have led to the construction of large KGs in the format \textit{$<$subject predicate object$>$} triples. However, due to the automatic construction, these KGs can be incomplete. Horn rule mining is a popular approach to address this issue~\cite{ref10}.

In this thesis we have looked at the problem of enhancing the quality of positive rules and subsequently improve the accuracy of their predictions, by revising the mined rules into the non-monotonic ones. We follow the ideas from~\cite{ref12} where the same problem was studied for KG containing only unary facts. Meanwhile, RUMIS tool implemented in our research generates negative rules from KG in nature format.

We have extended the results from~\cite{ref12} to KG revision in their original relational form and developed the RUMIS system, for learning nonmonotonic rules in KGs under the OWA. Subsequently, the chosen revisions are exploited to extend the original data, and thus, tackle the KG completion problem. Some experiments in the thesis are conducted for testing quality of rules generated by RUMIS and the results demonstrate that the proposed approach outperforms the state of the art KG rule learning systems w.r.t. to the quality of the made predictions.

\section{Future Directions}

We now discuss possible future directions.

\begin{itemize}
\item \textbf{Tackle language bias challenge.} In this work we fixed the language bias of the rules to be mined. Certainly, a promising and natural direction is to extend our results to further rule forms and make the language bias more flexible. More forms for the positive rules can be implemented, not only the specific form introduced in Chapter~\ref{chap:system}. The language bias can be manually specified by users, this setting will make the RUMIS less restrictive and diversify possible revisions. Another possible future direction is to add existential operators to the heads of the rules, e.g., \textit{$\exists$ Y hasParent(X, Y) $\leftarrow$ person(X)} can be taken into consideration.
\item \textbf{Enriching exception forms.} Currently RUMIS only supports one exception in the body and this exception is the relation between variables in the head or a unary atom. To extend the form of non-monotonic rules, we can increase the number of exceptions in the body of the revision. E.g., some interesting rule like \textit{isCitizenOf(X, Y) $\leftarrow$ bornIn(X, Y), not manager(X), not isAsiaCountry(Y)} can be mined in the updated version of the system.
\item \textbf{Optimizations.} Due to the large size of the KGs, data indexing is a time burden step in our implementation. Thus, a natural optimization direction is storing the facts in a database and building a web service to find (ab)normal sets of conjunctive relational queries. Thanks to this technique, we only build the service once at the beginning, and the time for experiment is significantly reduced.
\item \textbf{Try more rule measures.} Other predictive measures and exception evaluation methods can be tested to search for interesting nonmonotonic rules. A survey in~\cite{ref46} specifies a variety of choices for predictive rule measure where conviction is only one of them.
\item \textbf{N-ary facts.} In YAGO and IMDB datasets, the facts are three-dimensional, i.e., each of them contains a subject, a predicate and an object. The Wikidata Football dataset exploited in the experiments has the same property, i.e., it is extracted from simplified version of original Wikidata where all facts are projected to three-dimensional space. One possible future extension takes the n-ary facts, i.e., facts contain n-parameters into consideration. E.g., the five-dimensional fact \textit{$<$Ronaldo$>$ $<$playsFor$>$ $<$Manchester United$>$ $<$in$>$ $<$2008$>$} may appear in full Wikidata KG.
\item \textbf{Sample training KG randomly.} In our current experiment, every learning KG is chosen from the ideal KG by removing 20\% of facts for each binary predicate. This setting is restrictive because it tries to maintain the distribution of facts over the predicates. It is worth testing the quality of predictions generated by RUMIS if the training data is chosen randomly where the percentage of the retained facts is different for every relation.
\item \textbf{Optimize DLV.} DLV running time is a burden in the current system. Indeed, with very large KGs and 10 revised rules, it takes up to days to extend KG. Since completing a given KG is one of the main goals of the tool, optimizations of DLV tool should be studied to make sure the total run time performance is acceptable.
\item \textbf{Facts with probability.} For our experiment, the facts in the given KG are always true, this is not suitable in practice where some facts can be totally false. Indeed, modern KGs might contain incorrect triples, since their large parts are constructed automatically by information extraction techniques. As a possible future direction, probability can be assigned to facts in the training KG as the weight, and taken into account during the rule learning and revision. This results in new predicted facts with a certain confidence, represented as weights assigned to them.
\end{itemize}