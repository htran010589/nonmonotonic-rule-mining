\chapter{Evaluation}
\label{chap:eval}

In this chapter, we present the evaluation results of the RUMIS system. We focus on testing the quality of rule revisions in terms of the conviction measure, as well the prediction quality. The chapter is organized as follows. First, the setting of the experiment is discussed. Second, the quality of rulesets and the facts they predict are assessed. Finally, we present the runtime performance of the system RUMIS on different real-world datasets.

\section{Setting}

\textbf{Dataset.} To measure the quality of rules and their predictions, the ideal graph $\cG^i$, i.e. the KG containing all true facts about the real world, is required. However, constructing $\cG^i$ is obviously not possible. Hence, instead we treat the given graph as an approximation of the ideal KG $\cG^i_{\mi{appr}}$. To obtain the available (training) KG we remove from $\cG^i_{\mi{appr}}$ 20\% of the facts from every binary relation, and retain all unary facts in $\cG^i_{\mi{appr}}$. It is guaranteed that there is no isolated vertex in $\cG^a$, i.e the node which is not connected to any other nodes in the graph. In our experiment, YAGO3~\cite{ref28}, Wikidata Football~\footnote{\url{https://www.wikidata.org}} and IMDB~\footnote{\url{http://people.mpi-inf.mpg.de/~gadelrab/downloads/ILP2016}} datasets are used as the ideal KGs. YAGO3 covers a variety of domains and contains roughly 1.8 million entities, 38 predicates, and 20.7 million triples. Meanwhile, IMDB only focus on movie content collected from the IMDB website~\footnote{\url{http://imdb.com}}, there are 112 thousand entities, 38 predicates and 583 thousand triples in this KG. Besides, to construct the Wikidata Football for our experiments, we sample 1 million facts with 238 thousand entities and 443 predicates from football domain of the original KG. %In details, thanks to randomly choosing facts in the format \textit{$<$subject$>$ $<$memberOf$>$ $<$object$>$} where \textit{$<$object$>$} is in the list of manually chosen popular football clubs, famous footballers and coaches (seeding set) are sampled from Wikidata. After that, all the entities that can be visited from any node in the seeding set (chosen entities) by following paths (one or more consecutive links/edges/predicates in the original Wikidata KG) are collected for the ideal KG. These chosen entities and all possible links between them in the original Wikidata KG are sampled for the smaller scale. This new KG is expected to have specific domain where facts are related to football and sports.

Figure~\ref{fig:venn} depicts the ideal, approximated and available KGs as well the extended KGs $\cG_{\cR_{\mi{H}}}$, $\cG_{\cR_{\mi{NM}}}$ obtained by resp. applying $\cR_H$ and its revision $\cR_{NM}$ to $\cG$. The RUMIS system aims to tackle the KG completion problem by narrowing the difference between the $\cG_{\cR_{\mi{NM}}}$ and $ \cG^i$.

\begin{figure}[ht]
\centering
\includegraphics[width=0.55\textwidth]{figures/big_pic_exp}
\caption{The Ideal, Available and Extended KGs.}
\label{fig:venn}
\end{figure}

\textbf{Experimental Setup.} In our experiments, RUMIS is executed on a server which has Linux OS, 40 cores and 400GB RAM memory. As the data preparation, positive rules in the format \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)} are extracted from $\cG^a$ and ranked according to the \textit{absolute support} measure. The positive rule mining function of RUMIS (see Chapter~\ref{chap:system}) is exploited in this step. After that, the positive rules are revised following the approach presented in Chapter~\ref{chap:frame}. The \textit{conviction} measure is used as the $\mi{rm}$ rule measure. Various exception ranking earlier described strategies are realized in RUMIS. The resulting rule revisions are stored in $\mi{\cR_{N}}$, $\mi{\cR_{PM}}$, and $\mi{\cR_{OPM}}$, resp.

\section{Ruleset Quality}

We present the evaluation results of the rule quality assessment in Table~\ref{tab:rules_quality1} and~\ref{tab:rules_quality2} for YAGO, IMDB and Wikidata Football, resp. For every row in the tables, we fix the top-$\mi{k}$ ($k=5,30,$ ... $100$) positive rules $\cR_{H}$ that will be subsequently revised. Then the \textit{average conviction} of the rules and their revisions are found for YAGO, IMDB and Wikidata Football by using RUMIS. Naturally the quality of the revised rules is better than that of their positive versions w.r.t. conviction. Besides, in general while the average quality of every column has a decreasing trend with the appearance of rules having lower precision levels, the enhancement ratio between positive rules and their revisions increases and reaches the peak of $7\%$ and $10\%$ for top-100 IMDB and top-30 Wikidata rules, resp. The results show that the introduction of negated atoms significantly boost up the rule precision.

\begin{table}[ht]
\centering
\footnotesize
\renewcommand*{\arraystretch}{1.07}
\centering
\input{tables/rules_quality_yago_imdb}
\smallskip
\caption{The Average Quality of the Top Positive and Nonmonotonic Rules for YAGO, IMDB.}
\label{tab:rules_quality1}
%\vspace*{-1.7\baselineskip}
\end{table}

\begin{table}[ht]
\centering
\footnotesize
\renewcommand*{\arraystretch}{1.07}
\centering
\input{tables/rules_quality_wikidata}
\smallskip
\caption{The Average Quality of the Top Positive and Nonmonotonic Rules for Sample Wikidata.}
\label{tab:rules_quality2}
%\vspace*{-1.7\baselineskip}
\end{table}

The enhancement ratio trend between revisions of the three ranking methods and the top positive rules is shown in Figure~\ref{fig_1_5_yago} and~\ref{fig_1_5_imdb} for YAGO and IMDB, resp. One can observe that the rate has an uptrend, in general the more low quality Horn rules are added to the top ruleset, the higher is the improvement rate. The Naive ranking shows the best results w.r.t. the rule quality, which is obviously expected.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{figures/table_1_5_yago.jpeg}
\caption{The Average Quality Improvement Rate (\%) against Positive Rules in YAGO.}
\label{fig_1_5_yago}
\end{figure}

With IMDB dataset, the results of Naive and OPM ranking are approximately the same, it is good to see that the quality of OPM revisions are the best while minimizing number of conflicts is still taken into consideration for this method. Especially with top 100 rules, the discrepancy rate reaches the peak of more than 7\% improvement which is significantly larger than the statistics with the same data.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{figures/table_1_5_imdb.jpeg}
\caption{The Average Quality Improvement Rate (\%) against Positive Rules in IMDB.}
\label{fig_1_5_imdb}
\end{figure}

\section{Prediction Quality}

We now describe the evaluation procedure for estimating the predictive quality of the revised rules. Among top-50 (for IMDB, YAGO3) and top-300 (for Wikidata) positive rules mined from the KG, 5 are sampled as $\cR_H$ and then the revision procedure is applied for these rules. After that, the predictions of the rules from $\cR_H$ and their revisions are analyzed to evaluate our approaches.

To this end, these rulesets are applied to the learning KG $\cG$ and the corresponding predicted facts are generated by DLV tool~\cite{dlv}. Subsequently, we achieve $\cG_{\cR_\mi{H}}$, $\cG_{\cR_\mi{N}}$, $\cG_{\cR_\mi{PM}}$ and $\cG_{\cR_\mi{OPM}}$ for $\cR_H$ and revisions of $\cR_H$, resp. The statistics is shown in Table~\ref{tab:prediction_res}, where the first three columns indicate the head relations of the rules in $\cR_H$, the number of new predictions, i.e. predicted facts not included in $\cG$ and the part of these predictions which are outside $\cG^i_{\mi{appr}}$, resp. The statistics in the second and third columns is available for positive rules and all of their revisions.

\begin{table}[ht]
\centering
\input{tables/rules_predictions}
\smallskip
\caption{New Facts Predicted by the Rulesets for IMDB (\textit{I}), YAGO (\textit{Y}) and Wikidata Football (\textit{W}).}
\label{tab:prediction_res}
%\vspace*{-1.7\baselineskip}
\end{table}

One can see that not many predicted facts are included in $\cG^i_{\mi{appr}}$ ($\approx$9\% for IMDB and $\approx$2\% for YAGO ideal KGs). This can be explained by the fact that YAGO is a highly incomplete general purpose KG. Moreover, it is crucial to note that the sampled Horn rules and their revisions generate approximately the same number of correctly predicted facts which are present in $\cG^i_{\mi{appr}}$. More specifically, in YAGO $\cG_{\mi{\cR_H}}\backslash \cG_{\mi{\cR_{PM}}} \cap \cG^i_{\mi{appr}}=\emptyset$ means that the grey region has nothing in common with the approximated ideal KG in Figure~\ref{fig:venn}, in other words, the addition of exceptions does not lead to the removal of correct predictions from $\cG^i_{\mi{appr}}$.

To guarantee the fairness of the comparison between predictions generated by different rulesets, it is necessary to keep the $\cR_H$ not totally imprecise. Indeed, provided that the sampled positive rules always generate incorrect facts, inserting arbitrary negated atoms may filter out some incorrect predictions, resulting in the rule enhancement, yet the rules themselves would still be of poor quality. Furthermore, observe that the number of predictions made by $\cR_H$ outside $\cG^i_{\mi{appr}}$ (third column of Table~\ref{tab:prediction_res}) is rather large. To verify these predictions, unfortunately no ground truth is available. Thus, we have to manually check the generated facts using the Internet resource. Since the number of the facts to be checked is huge, we propose to randomly select maximum 20 new predictions for each head predicate in $\cR_H$ and verify them. For the IMDB, YAGO and Wikidata Football, 70\%, 30\% and 55\% of predictions respectively turned out to be in deed correct. This shows that the quality of the positive rules that we start with is acceptable.

Since the size of the set difference between predictions made by $\cR_H$ and extended by applying $\cR_H$ and its revisions is also huge, we have to proceed further with sampling to evaluate the predictive quality of the revision. Here for each head relation from the set differences $\cG_{\cR_\mi{H}}\backslash \cG_{\cR_{\mi{N}}}$, $\cG_{\cR_\mi{H}}\backslash \cG_{\cR_{\mi{PM}}}$ and $\cG_{\cR_\mi{H}}\backslash \cG_{\cR_{\mi{OPM}}}$, 10 facts have been randomly sampled for manual check. In the last column of the Table~\ref{tab:prediction_res}, the proportion of incorrect facts in the difference sets are presented. These facts are called ``correctly removed", since they correspond to false prediction made by $\cR_H$ but avoided by the respective revisions (the grey region in in Figure~\ref{fig:venn}). For the IMDB dataset, among all the revision strategies, OPM ranking always performs best with 57.75\% and 97.5\% correctly removed predictions for IMDB and Wikidata Football, resp. Meanwhile, all the rankers demonstrate the same results (85\%) for the YAGO KG. Since the predictive power of the positive rules in IMDB is better than those in YAGO, the revision of the latter makes a more visible impact than the former.

\section{Running Times}

RUMIS mines 100 positive rules and their $\mi{EWS}$s from IMDB and YAGO in 7 and 68 seconds, resp. The mean number of exceptions for each positive rule is 1600 for the former and 10900 for the latter. With IMDB, the exceptions for Naive, PM and OPM strategies are ranked within 9, 62 and 24 seconds, resp. Meanwhile in YAGO, these numbers are 45, 177 and 112 seconds with the same ranking function. As regards the step of applying final revisions to the KG $\cG$, the DLV tool finishes after 8 and 310 seconds with IMDB and YAGO, resp.

As regards sampled Wikidata, 300 Horn rules and their EWSs are explored in 20 and 173 seconds, resp. The exceptions are ranked in resp. 1.5, 115 and 31 minutes by Naive, Pm, OPM methods and training KG is extended in 3 minutes by using DLV in average.

\begin{figure}[t]
    \centering
   
    \vspace{-.2cm}
    \begin{tabular}{l}
 {\scriptsize
        $\mi{r_1: writtenBy(X, Z)}  \leftarrow
        \mi{hasPredecessor(X, Y)},\mi{writtenBy(Y, Z)},$ $ \textbf{not}$  $\mi{american\_film(X)} $}\\        
       {\scriptsize 
$\mi{r_2:  actedIn(X, Z)}  \leftarrow
        \mi{isMarriedTo(X, Y)},\mi{directed(Y, Z)},$ $ \textbf{not}$  $\mi{silent\_film\_actor(X)} $} \\
          {\scriptsize 
$\mi{r_3:  isPoliticianOf(X, Z)}  \leftarrow
        \mi{hasChild(X, Y)}{,}\mi{isPoliticianOf(Y, Z)}{,}$$ \textbf{not}$  $\mi{vicepresidentOfMexico(X)} $} \\
 \end{tabular}            
    \caption{Examples of the Revised Rules}
 \label{fig:examplerules}
 \vspace{-.4cm}
\end{figure}

\textbf{Example rules.} Some interesting examples are presented in the Figure~\ref{fig:examplerules}. Here 
the rule $\mi{r_1}$ mined from IMDB dataset indicates that normally movies in the same series are written by the same writer except the American movies. The rule $\mi{r_3}$ generated from YAGO reveals an interesting pattern from domain politics, i.e, typically fathers and sons are politicians in the same country unless the fathers are Mexican vice-presidents.