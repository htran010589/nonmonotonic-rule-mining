\chapter{Evaluation}
\label{chap:eval}

In this chapter, the evaluation result of the current work is presented after running experiment feature of RUMIS. We focus on testing the quality of rule revisions in terms of conviction measure, and how precise are their predictions. The chapter is organized as follows, first, the setting of the experiment is discussed. Second, the quality of rulesets and their predicted facts are assessed. Finally, we present the runtime performance of RUMIS with different datasets.

\section{Setting}

\textbf{Dataset.} To measure the quality of rules and their predictions, the occurrence of ideal graph $\cG^i$, i.e. the ideal KG containing all true facts from real world, is necessary. Nonetheless, achieving $G^i$ is infeasible and impracticable. Hence, some KG $\cG^i_{\mi{appr}}$ with a large number of true facts can be exploited as an approximated version for $\cG^i$. Besides, the learning KG $\cG^a$ is created by deleting 20\% of the triples over every binary relation, and retaining all unary facts in $\cG^i_{\mi{appr}}$. It is guaranteed that there is no isolated vertex, i.e the node which does not connect to any other nodes in $\cG^a$. In our experiment, YAGO3~\cite{yago3} and IMDB~\footnote{\url{http://people.mpi-inf.mpg.de/~gadelrab/downloads/ILP2016}} datasets are used as the ideal KGs. The former covers a variety of domains and contains roughly 1.8 million entities, 38 predicates, and 20.7 million triples. Meanwhile, the latter only focus on movie content collected from the IMDB website~\footnote{\url{http://imdb.com}}, there are 112 thousand entities, 38 predicates and 583 thousand triples in this KG.

[describe more for Venn diagram here]
Ideally,  the set difference between $\cG_{\cR_{\mi{NM}}}$ and $ \cG^i$ should be minimized (see  Fig.~\ref{fig:venn} for illustration).

\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/big_pic_exp}
\caption{Relations between the ideal, approximated and available slices of a KG.}
\label{fig:venn}
\end{figure}

\textbf{Setup.} In our experiment, RUMIS is executed on a server which has Linux OS, 40 cores and 400GB RAM memory. As the data preparation, positive rules with the form~\ref{form2} are explored from $\cG^a$ and ranked according to the \textit{absolute support} measure. The positive rule mining function of RUMIS is exploited in this step~\ref{chap:system}.

After that, the positive rules are revised with the methodology presented in the Section~\ref{chap:meth}, here \textit{conviction} is used for the $\mi{rm}$ rule measure.

Exceptions for each positive rule are ranked and the optimal one in terms of the measure score is added to the final set. This ranking feature of RUMIS is applied for different strategies such as \emph{Naive}, \emph{PM}, and \emph{OPM}, and store the results in $\mi{\cR_{N}}$, $\mi{\cR_{PM}}$, and $\mi{\cR_{OPM}}$, resp.

\section{Ruleset quality}

For every row in the Table~\ref{tab:rules_quality}, we fix the top-$\mi{k}$ ($k=5,30,$ ... $100$) positive rules $\cR_{H}$ from the preparation step. Then the \textit{average conviction} of the rules and their revisions are found for YAGO and IMDB by using RUMIS. It can be seen that the quality of the positive ruleset is always improved in terms of the average conviction thanks to the revision process. In addition, while the average quality of every column has a decreasing trend with the appearance of rules having lower precision level, the enhancement ratio between positive rules and their revisions steps up and reaches the peak of $7.6\%$ for top-100 IMDB rules. That means with low quality Horn rules, the introduction of negated atoms significantly boost up the their precision.

\begin{table}[ht]
\centering
\footnotesize
\renewcommand*{\arraystretch}{1.07}
\centering
\input{tables/rules_quality}
\smallskip
\caption{The average conviction for the \textit{top-k} Horn rules and their revisions.}
\label{tab:rules_quality}
\vspace*{-1.7\baselineskip}
\end{table}

\section{Prediction quality}

Among top-50 of positive rules mined in the preparation step, 5 of them are sampled as $\cR_H$ and then the methodology is applied to the datasets. After that, the predictions of the $\cR_H$ and their revisions are analyzed to evaluate the proposed theory.

To this end, these rulesets are applied to the learning KG $\cG^a$ using DLV tool~\cite{dlv}. Subsequently, we achieve $\cG_{\cR_\mi{H}}$, $\cG_{\cR_\mi{N}}$, $\cG_{\cR_\mi{PM}}$ and $\cG_{\cR_\mi{OPM}}$ for $\cR_H$ and their revisions, resp. Some statistics for that is shown on Table~\ref{tab:prediction_res} where the first three columns indicate head relations in the $\cR_H$, the
quantity of new predictions, i.e. predicted facts not included in $\cG^a$ and the part of these predictions which are outside $\cG^i_{\mi{appr}}$. The statistics in the second and third columns is available for all raking options.

\begin{table}[ht]
\centering
\input{tables/rules_predictions}
\smallskip
\caption{New facts predicted by the rulesets for IMDB (\textit{I}) and YAGO (\textit{Y}).}
\label{tab:prediction_res}
\vspace*{-1.7\baselineskip}
\end{table}

It can be inferred that there are not many predicted facts included in $\cG^i_{\mi{appr}}$ ($\approx$9\% in IMDB and $\approx$2\% in YAGO ideal KGs). This illustrates our expectation since the latter is a general purpose KG and it misses a lot of facts. Also, it is crucial to see that the sampled Horn rules and their revisions generate approximately the same quantity of correctly predicted facts which are confirmed in $\cG^i_{\mi{appr}}$. E.g., in YAGO $\cG_{\mi{\cR_H}}\backslash \cG_{\mi{\cR_{PM}}} \cap \cG^i_{\mi{appr}}=\emptyset$ holds true, that means the green region has nothing in common with the approximated ideal KG in Figure~\ref{fig:venn}. In other words, the appearance of exceptions does not remove any correct facts predicted by $\cR_H$ according to $\cG^i_{\mi{appr}}$.

To guarantee the fairness of the comparison between predictions generated by different rulesets, it is necessary to keep the $\cR_H$ not totally false. Because once the sampled positive rules always generate incorrect facts, inserting arbitrary negated atoms may filter out some incorrect ones, followed by the enhancement of the revision precision. Besides, the numbers in the third column of Table~\ref{tab:prediction_res} according to $\cR_H$ are big, that means there are a lot of facts generated by this ruleset outsite the approximated ideal KG, and no ground truth is available to verify them. Thus, we have to use the Internet resource has to manually check the generated facts. Since the quantity of them is huge, we propose to randomly select 20 new predictions for each head predicate in $\cR_H$. With the selected IMDB facts, the accuracy is 70\% while this number of YAGO dataset is 30\%. It can be inferred that the quality of mined positive rules is not totally false and fulfills the above requirement.

Since the size of difference sets between KGs extended by applying $\cR_H$ and its revisions to $\cG^a$ is also huge, we have to apply the sampling again. Here for each head relation from the difference sets $\cG_{\cR_\mi{H}}\backslash \cG_{\cR_{\mi{N}}}$, $\cG_{\cR_\mi{H}}\backslash \cG_{\cR_{\mi{PM}}}$ and $\cG_{\cR_\mi{H}}\backslash \cG_{\cR_{\mi{OPM}}}$, 10 facts are randomly sampled for manual check. In the last column of the Table~\ref{tab:prediction_res}, the proportion of false facts in the difference sets are presented. These facts are called ``correctly removed" and correspond to the grey region in in Figure~\ref{fig:venn}. With IMDB dataset, among all the strategies, OPM ranking is the best one with 57.75\% correctly removed predictions. Meanwhile, all the rankers share the same quality of 85\% correctly removed facts in YAGO. Because the predictive power of the rules in IMDB is better than those in YAGO, the revisions of the latter play a higher impact than the former.

\section{Running times.}

RUMIS mines 100 positive rules and their $\mi{EWS}$s from IMDB and YAGO in 7 and 68 seconds, resp. The mean number of exceptions for each positive rule is 1600 for the former and 10900 for the latter. With IMDB, the exceptions for Naive, PM and OPM strategies are ranked within 9, 62 and 24 seconds, resp. Meanwhile in YAGO, these numbers are 45, 177 and 112 seconds with the same ranking function. As regards the step of apply final revisions to the KG $\cG^a$, the DLV tool finishes after 8 and 310 seconds with IMDB and YAGO, resp.

\begin{figure}[t]
    \centering
   
    \vspace{-.2cm}
    \begin{tabular}{l}
 {\scriptsize
        $\mi{r_1: writtenBy(X, Z)}  \leftarrow
        \mi{hasPredecessor(X, Y)},\mi{writtenBy(Y, Z)},$ $ \textbf{not}$  $\mi{american\_film(X)} $}\\        
       {\scriptsize 
$\mi{r_2:  actedIn(X, Z)}  \leftarrow
        \mi{isMarriedTo(X, Y)},\mi{directed(Y, Z)},$ $ \textbf{not}$  $\mi{silent\_film\_actor(X)} $} \\
          {\scriptsize 
$\mi{r_3:  isPoliticianOf(X, Z)}  \leftarrow
        \mi{hasChild(X, Y)}{,}\mi{isPoliticianOf(Y, Z)}{,}$$ \textbf{not}$  $\mi{vicepresidentOfMexico(X)} $} \\
 \end{tabular}            
    \caption{Examples of the revised rules}
 \label{fig:examplerules}
 \vspace{-.4cm}
\end{figure}

[paraphase]

\textbf{Example rules.} Some interesting examples of the final revisions are presented in the Figure~\ref{fig:examplerules}. Here 
the $\mi{r_1}$ mined from IMDB dataset indicates that normally movies in the same series share the same writer except the American ones. In addition, $\mi{r_3}$ generated from YAGO reveals an interesting story about politics. To be specific, typically  the pattern that fathers and sons are politicians in the same country is popular, but this does not hold true with Mexican vice-presidents.