\chapter{RUMIS - Nonmonotonic Rule Mining System}\label{chap:system}
\label{chap:system}

RUMIS stands for Nonmonotonic Rule Mining System which is a tool developed within the current thesis. The RUMIS system aims at revising Horn rules to nonmonotonic ones under the OWA. This chapter describes in details the practical implementation of the theory framework presented in Chapter~\ref{chap:frame} and uses background definitions in Chapter~\ref{chap:back}. First, we present the system overview. Second, the implementations of main components of the system are described. Finally, we discuss the system usage.

\section{System Overview}
\label{sec:overview}

\begin{figure}[h]
\centering
\includegraphics[width=0.81\textwidth]{figures/system_overview}
\caption{Components of RUMIS}
\label{system_overview}
\end{figure}

There are six components in RUMIS as presented in Figure~\ref{system_overview} with arrows indicating the data flow from input to output. RUMIS mines the nonmonotonic rules from the original graph in the following steps:

\begin{itemize}
\item In (1), a KG $\cG$ is passed to RUMIS as input. It is then stored and indexed by Component 1 which is exploited to speed up the computation in the next steps.
\item In (2), Horn rules are mined by Component 2 from facts indexed into RUMIS.
\item In (3), normal and abnormal instance sets are found by Component 3 based on the KG $\cG$ and the Horn rules mined in the previous step.
\item Given normal and abnormal sets known for each Horn rule, (4) represents the step of finding exception witness sets in Component 4.
\item In (5) and (6), exception candidates are ranked in Component 6 using a measure provided by Component 5. RUMIS allocates a separate component for measure plugin to enable flexibility of ranking criteria.
\item In (7), RUMIS returns the best revision for the Horn ruleset as output.
\end{itemize}

\section{Implementation}

In this section, the main components mentioned in Section~\ref{sec:overview} are described in details.

\subsection{Data Indexing}
\label{data_indexing}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{figures/data_indexing}
\caption{RUMIS Data Indexer}
\label{data_indexing}
\end{figure}

Since the KG $\cG$ can be large, computation of (ab)normal instances as well as exception witness sets is time-consuming. To overcome this issue, data indexing is exploited. In a traditional search engine setting, terms such as words, n-grams are indexed into the system and their corresponding posting lists~\cite{ref47} are a collection of documents containing them. We exploit the same intuition in our work. More specifically, RUMIS treats every fact as a document and the terms are subjects, predicates, objects or combinations of them.

For example, Figure~\ref{data_indexing} presents the data indexed from part of a KG in Figure~\ref{fig1.1}. Hence, given the predicate \textit{isMarriedTo}, one can determine a set of subject-object pairs corresponding to it, i.e., \textit{$<$Brad, Ann$>$} and \textit{$<$John, Kate$>$}. Similarly, given a subject and a relation, the set of objects can be retrieved. For example, we can get ``Berlin" from the question ``Where does Ann live" based on the data indexing model in Figure~\ref{data_indexing}. In this example, we index the combination of subject and predicate \textit{$<$Ann, livesIn$>$} to efficiently retrieve the result \textit{Berlin}. More formally, the Data Indexing provides the following functions that are exploited in the rest of the components:

\begin{itemize}
\item \textit{getPredicateSubjectSet($\cG$, object)} returns a set of predicate-subject pairs corresponding to a given \textit{object} in the KG $\cG$.
\item \textit{getPredicateSet($\cG$, subject, object)} returns a set of predicates corresponding to the input \textit{subject, object} entities.
\item \textit{getSubjectSet($\cG$, predicate, object)} retrieves a set of subjects for a given \textit{predicate} and an \textit{ object}.
\item \textit{getSubjectObjectSet($\cG$, predicate)} outputs a set of \textit{subject-object} pairs for a given \textit{predicate}.
\end{itemize}

\subsection{Positive Rule Mining}

\IncMargin{1.5em}
\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{KG $\cG$}
\Output{Set of positive rules of the form \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)}}
\BlankLine
$absSupp \leftarrow \{\}$\\
\BlankLine
\ForEach{Triple $YqZ$ in $\cG$} {
    \BlankLine
	$pXSet \leftarrow getPredicateSubjectSet(\cG, Y)$\\
	\ForEach{$pX$ in $pXSet$} {
		$hSet \leftarrow getPredicateSet(\cG, X, Z)$\\
		\ForEach{$h$ in $hSet$} {
			$absSupp[hpq]++$\\
		}
	}
}
\BlankLine
Sort $hpq$ in a decreasing order of $absSupp[hpq]$\\
\Return $absSupp$\\
\caption{Positive Rule Mining}
\label{algo1}
\end{algorithm}
\DecMargin{1.5em}

Positive Rule Mining component computes a set of rules of the form \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)}, whose \textit{absolute support} exceeds a given threshold. Existing tools from the literature can be exploited for this component, however, due to technical issues we have re-implemented the Horn rule learning based on Algorithm~\ref{algo1}. The steps of this algorithm are described as follows. First, \textit{absSupp} defined in line 1 is initialized for storing the absolute support of patterns. In the loop (2), for each triple \textit{$<$Y q Z$>$} from the KG $\cG$, using the \textit{getPredicateSubjectSet} function in the Data Indexing component, a set of pairs \textit{X, p} is found in (3) such that \textit{$<$X p Y$>$} is in $\cG$. Then, from line 5 to 6, with \textit{X, Z} found in the previous step, RUMIS searches for every relation $h$ s.t. \textit{$<$X h Z$>$} is in $\cG$. At this point, it is guaranteed that \textit{h(X, Z), p(X, Y), q(Y, Z)} holds. After that, in line 7, the absolute support of the considered rule is increased by $1$. Finally, all rules are sorted in decreasing order of their absolute support (line 11).

\subsection{Normal and Abnormal Set Mining}

\IncMargin{1.5em}
\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{KG $\cG$, $h$, $p$, $q$ predicates in the rule \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)}}
\Output{Normal and abnormal sets of the rule \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)}}
\BlankLine
$NS \leftarrow \{\}$\\
$ABS \leftarrow \{\}$\\
$YZSet \leftarrow getSubjectObjectSet(\cG, q)$\\
\BlankLine
\ForEach{Pair $YZ$ in $YZSet$} {
    \BlankLine
	$XSet \leftarrow getSubjectSet(\cG, p, Y)$\\
	\ForEach{$X$ in $XSet$} {
	\uIf{$XhZ$ is in $\cG$} {
		Add $XZ$ to $NS$\\
	}
	\uElse {
		Add $XZ$ to $ABS$\\
	}
	}
}
\BlankLine
\Return $NS$ and $ABS$\\
\caption{Normal and Abnormal Set Mining}
\label{algo2}
\end{algorithm}
\DecMargin{1.5em}

This component computes normal and abnormal instance sets for a given rule and a KG as described in Algorithm~\ref{algo2}. First, in lines 1 and 2, variables for (ab)normal sets are initialized. Second, for each pair \textit{Y, Z} s.t. \textit{$<$Y q Z$>$} is in $\cG$, a set of $X$ for which \textit{$<$X p Y$>$} is in the KG is found based on the Data Indexing component (line 3 to 5). At this point, it is guaranteed that \textit{X, Y, Z} satisfy the body of the given rule. Finally, for every $X$ found in the previous step, we verify whether \textit{$<$X h Z$>$} is in the KG or not. If it is in $\cG$, then \textit{$<$X Z$>$} is added to the normal set, otherwise it is added to the abnormal set.

\subsection{Exception Witness Set Mining}

\IncMargin{1.5em}
\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{KG $\cG$, $h$, $p$, $q$ predicates of the rule \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)}}
\Output{Exception witness set of the rule \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)}}
\BlankLine
$NS \leftarrow getNormalSet(\cG, h, p, q)$\\
$ABS \leftarrow getAbnormalSet(\cG, h, p, q)$\\
$EWS^+ \leftarrow \{\}$\\
$EWS^- \leftarrow \{\}$\\
\BlankLine
\ForEach{Pair $XZ$ in $ABS$} {
	$pSet \leftarrow getPredicateSet(\cG, X, Z)$\\
	$EWS^+$ $\leftarrow$ $EWS^+$ $\cup$ $pSet$\\
}
\ForEach{Pair $XZ$ in $NS$} {
	$pSet \leftarrow getPredicateSet(\cG, X, Z)$\\
	$EWS^-$ $\leftarrow$ $EWS^-$ $\cup$ $pSet$\\
}
$EWS$ $\leftarrow$ $EWS^+$ $\setminus$ $EWS^-$\\
\Return $EWS$\\
\caption{Exception Witness Set Mining}
\label{algo3}
\end{algorithm}
\DecMargin{1.5em}

We now describe the algorithm for exception witness set construction (Algorithm~\ref{algo3}). Given a KG and a positive rule, this component computes all unary and binary exception candidates. To simplify the presentation, Algorithm~\ref{algo3} focuses only on computing binary exceptions, analogously, unary ones can be mined. More specifically, our goal is to find exceptions of the form \textit{e(X, Z)} which are then inserted into the body of \textit{h(X, Z) :- p(X, Y), q(Y, Z)} to get its revised rule \textit{h(X, Z) :- p(X, Y), q(Y, Z), not e(X, Z)}. Algorithm~\ref{algo3} proceeds as follows. First, normal and abnormal instance sets are found in line 1 and 2 by exploiting Algorithm~\ref{algo2}. Second, the variables $EWS^+$ and $EWS^-$ which store a set of relations between \textit{$<$X Z$>$} in the abnormal and normal sets respectively are created (line 3 and 4). Third, for every pair \textit{$<$X Z$>$} in the abnormal set, all relations between \textit{X} and \textit{Z} are added to the $EWS^+$ (line 5 to 8). After that, a similar procedure is applied to $EWS^-$ in lines 9 to 12. Finally, exception witness set $EWS$ is constructed as the difference between $EWS^+$ and $EWS^-$ (line 13), which forms the output of the algorithm in line 14.

\subsection{Measure Plugin}

The RUMIS system is supplied with the measure plugin which allows the user to conveniently specify various rule quality criteria. In the current implementation, confidence and conviction are supported. While the former is well-suited for descriptive purposes, the latter is accepted as a possible measure for estimating rule's predictive capabilities~\cite{ref46}. Since the main concern of this thesis is the Horn rule revision and subsequent application of the revised set for predicting possibly missing facts in the original KG, the conviction is the most suitable measure, which has thus been implemented within the RUMIS system.

\subsection{Exception Ranking}
\label{intuition_er}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/ranking}
\caption{(Ordered) Partial Materialization Ranking}
\label{pm_ranking}
\end{figure}

\IncMargin{1.5em}
\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{KG $\cG$, set of positive rules $\cR_{H}$}
\Output{Set of best revisions $R_{NM}$ for the given positive rules $R_H$}
\BlankLine
$R_{NM} \leftarrow \{\}$\\
\BlankLine
\ForEach{Rule $r$ in $R_H$} {
	$\cG' \leftarrow \cG$\\
	\ForEach{Rule $r''$ in $R_H$, $r''$ is different from $r$} {
		Generate safely predicted facts of $r''$ w.r.t. $\cG$ then index these facts to $\cG'$\\
	}
	Rank exceptions of $r$ based on $\cG'$, choose $r'$ as the best revision of $r$\\
	Add $r'$ to $R_{NM}$\\
}
\Return $R_{NM}$\\
\caption{PM Ranking}
\label{bf_pm_ranking_algo}
\end{algorithm}
\DecMargin{1.5em}

We now describe the exception ranking procedure in details which is illustrated in Figure~\ref{pm_ranking} on the PM ranker.

\begin{itemize}
\item From a given KG $\cG$ and a set of Horn rules, EWS mining (Algorithm~\ref{algo3}) is executed to find exception candidates for each Horn rule in (1). As shown in Figure~\ref{pm_ranking}, a Horn rule may have several exceptions.
\item (2) shows that revisions with exceptions (rules $r3, r4, r5, ...$) are used to infer new facts from the original KG.
\item $\cG'$, i.e., the original KG $\cG$ with new predicted facts, is exploited to rank exceptions for each Horn rule in (3). The combination of steps (2) and (3) describes the interaction of different rules, i.e, facts generated by other revisions are taken into account to measure quality of a particular nonmonotonic rule.
\item (4) reflects the process in which exceptions are ranked by Component 5 according to $\cG'$ from (3), and the best exception is chosen for the addition to the final revision set.
\end{itemize}

\textbf{PM ranking.} Algorithm~\ref{bf_pm_ranking_algo} describes PM ranking procedure, which takes as input a KG and a set of positive rules $R_H$ and outputs a set $R_{NM}$ of their revisions. In line 1, a set of revised rules is initialized as an empty set. After that, for each positive rule $r$ in $R_H$, we clone the original KG $\cG$ to a new KG $\cG'$. Then all other rules in $R_H$ are exploited to generate their safely predicted facts which are subsequently added to $\cG'$ (line 2 to 6) by Data Indexing component. Next, in lines 7 and 8, based on the new KG $\cG'$, exceptions of $r$ are ranked and the best revision is added to $R_{NM}$. Finally, we return $R_{NM}$ as an output of the algorithm.

\textbf{OPM ranking.} This function is similar to PM ranking. The major difference is the way how the rules are selected for KG expansion. At the initial step, Horn rules are sorted in decreasing order of conviction measure. After that, only safely predicted facts from previous rules are exploited to assess the quality of a given rule $r$. In the OPM ranking, the order of Horn rules matters, i.e., the higher is the conviction of a Horn rule, the more prominent is its impact on other rules.

%\IncMargin{1.5em}
%\begin{algorithm}[H]
%\DontPrintSemicolon
%\SetAlgoLined
%\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%\Input{KG $\cG$, set of positive rules $\cR_{H}$}
%\Output{Set of best revisions $R_{NM}$ for the given positive rules $R_H$}
%\BlankLine
%$\cG' \leftarrow \cG$\\
%$R_{NM} \leftarrow \{\}$\\
%Sort all rules in $R_H$ according to the decreasing order of conviction\\
%\BlankLine
%\ForEach{Rule $r$ in $R_H$} {
%	Rank exceptions of $r$ based on $\cG'$, choose $r'$ as the best revision of $r$\\
%	Add $r'$ to $R_{NM}$\\
%	Generate safely predicted facts of $r$ w.r.t. $\cG$ then index these facts to $\cG'$\\
%}
%\Return $R_{NM}$\\
%\caption{OPM Ranking}
%\label{opm_ranking_algo}
%\end{algorithm}
%\DecMargin{1.5em}

\section{Optimization}

\IncMargin{1.5em}
\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{KG $\cG$, set of positive rules $\cR_{H}$}
\Output{Set of best revisions $R_{NM}$ for the given positive rules $R_H$}
\BlankLine
$\cG' \leftarrow \cG$\\
$R_{NM} \leftarrow \{\}$\\
\BlankLine
\ForEach{Rule $r$ in $R_H$} {
	Generate safely predicted facts of $r$ w.r.t. $\cG$ then index these facts to $\cG'$\\
}
\BlankLine
\ForEach{Rule $r$ in $R_H$} {
	Generate safely predicted facts of $r$ w.r.t. $\cG$ then remove these facts' indexes from $\cG'$\\
	Rank exceptions of $r$ based on $\cG'$, choose $r'$ as the best revision of $r$\\
	Add $r'$ to $R_{NM}$\\
	Generate safely predicted facts of $r$ w.r.t. $\cG$ then index these facts to $\cG'$ (reverses step in line 7)\\
}
\Return $R_{NM}$\\
\caption{PM Ranking}
\label{pm_ranking_algo}
\end{algorithm}
\DecMargin{1.5em}

In Algorithm~\ref{bf_pm_ranking_algo}, the KG $\cG$ cloning and indexing procedures of the safely predicted facts are executed $|R_H|$ and $|R_H|^2$ times, respectively. Since the number of facts in $\cG$ and positive rules can be large, these operations are time-consuming. We now discuss possible optimizations of Algorithm~\ref{bf_pm_ranking_algo}, which concern the KG cloning and data indexing operations. In Algorithm~\ref{pm_ranking_algo}, we present possible refinements of Algorithm~\ref{bf_pm_ranking_algo} for the PM ranking.

In lines 1 and 2 of the Algorithm~\ref{pm_ranking_algo}, we clone the original graph $\cG$ to $\cG'$ and create an empty nonmonotonic ruleset $R_{NM}$. After that, from line 3 to 5, for every Horn rule $r$ in $R_H$, its safely predicted facts are added to $\cG'$ using the Data Indexing component.

Now RUMIS is ready to refine PM ranking. In line 7, for each Horn rule $r$, indexes of its safely predicted facts are removed from $\cG'$. This step is needed in order to make sure that safely predicted facts of all other rules apart from $r$ are exploited to determine the quality of $r$ (line 8). This witnesses that the interaction between nonmonotonic rules is taken into consideration during the ranking. After that, the best revision of $r$ is added to the final result in line 9. Line 10 shows a step that reverses what is done in line 7, i.e., safely predicted facts of the rule $r$ based on $\cG$ are added to $\cG'$ again. This guarantees that the same state in the next iteration can be processed with a new rule.

With the optimized algorithm, we only need $O(|R_H|)$ operations for indexing new predicted facts and one operation for cloning the KG. This is a significant improvement compared to the original version of the algorithm. The difference is visible in practice if the KG is large and many rules are considered.

\section{Usage}

RUMIS~\footnote{\url{https://github.com/htran010589/nonmonotonic-rule-mining}} is developed and currently tested in Linux, we may extend it to Windows in the future. The system requires the installation of Java 8 as well as the DLV~\footnote{\url{http://www.dlvsystem.com/html/DLV_User_Manual.html}} tool. RUMIS supports the following main tasks:

\begin{itemize}
\item \textit{Training data generation:} Given the ideal KG, the training KG is constructed automatically by removing 20\% of facts from $\cG$ for every binary predicate.
\item \textit{Horn rule mining:} With the training KG, a list of Horn rules of the form \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)} can be learned exploiting Algorithm~\ref{algo1}.
\item \textit{Nonmonotonic rule mining:} Based on the training KG, exceptions for each positive rule are ranked and the best one is chosen to generate the revision.
\item \textit{Automatic experiment:} With the KGs $\cG$ and $\cG^i$, the experiment can be executed to measure the quality of revision sets and predict new facts from the training data.
\end{itemize}

In the rest of this section, we list the command line options of RUMIS and explain them in details.

%\subsection{Setting}
%
%\textbf{DLV.} This tool is used to extend a KG given a set of Horn or nonmonotonic rules at hand. RUMIS exploits DLV in its experiment function, i.e, a feature of RUMIS for evaluating result.
%
%\textbf{Predicate Ratio and Learning KG.} RUMIS provides a function for creating a learning KG based on a predicate ratio. For example, given the KG and 0.8 as the ratio, then 80\% facts of every binary predicate are retained in the learning KG. The original KG is called the approximated ideal one.
%
%\textbf{Working Folder.} Working folder is a location for experiment where we have learning and approximated ideal KGs in SPO format (\textit{training.data.txt} and \textit{ideal.data.txt}, resp.), positive rule and sampled positive rule files (\textit{horn-rules.txt} and \textit{selected.horn-rules.txt}, resp.) in format~\ref{form2}. Besides, DLV binary file\footnote{\url{http://www.dlvsystem.com/files/dlv.x86-64-linux-elf-static.bin}} should be downloaded and renamed to \textit{dlv.bin} in this folder. The \textit{selected.horn-rules.txt} containing a subset of rules in \textit{horn-rules.txt}, it is only necessary if we want to revise some rules in this file.

\textbf{Command line options.} The RUMIS system supports the following options:

\begin{itemize}
\item \textit{-d:} This flag enables DLV in order to extend the KG.
\item \textit{-e=[execution function]:} This option requires a string as a function for execution. Here \textit{new, pos, neg, exp} correspond to creating a new learning KG, positve and nonmonotonic rule mining and conducting the experiment, respectively.
\item \textit{-f=[working folder path]:} With this option, experiment folder path can be specified.
\item \textit{-h:} Option used to retrieve the help menu of the system.
\item \textit{-l=[KG file path]:} This requires a file path to the graph in the SPO format, which enables users to choose the learning data.
\item \textit{-o=[predicate ratio]:} With this option, one can fix the percentage of facts to be removed for the creation of a learning KG.
\item \textit{-p=[Horn rule file path]:} This requires a string as an Horn rule file path. Each line in this file is a positive rule in the form \textit{h(X, Z) $\leftarrow$ p(X, Y), q(Y, Z)}.
\item \textit{-r=[ranking]:} Option allows one to specify the ranking type, i.e., 0, 1, 2 standing for Naive, PM, OPM ranking, respectively.
\item \textit{-s:} This flag is used to enable sampling of the positive rules.
\item \textit{-t=[number of top Horn rules]:} This requires an integer as a number of positive rules with the top absolute support that will be considered for revision.
\end{itemize}

\textbf{Command examples.} First of all, please download the repository\footnote{\url{https://github.com/htran010589/nonmonotonic-rule-mining}}, and then uncompress \textit{data/sample.imdb.txt.zip} to get \textit{sample.imdb.txt} file. In the next step, the repository root folder should be located: \textit{\$ cd nonmonotonic-rule-mining}. Now we are ready to present some command examples for using the RUMIS system.

\textit{Training data generation.} A learning KG of the IMDB sample dataset can be generated with predicate ratio of 80\%: \textit{\$ java -jar rumis-1.0.jar -e=new -l=data/sample. imdb.txt -o=0.8 1$>$training.sample.imdb.txt}. The generated file \textit{training.sample.imdb.txt} stores the learning KG.

\textit{Horn rule mining.} The following command is used for executing IMDB Horn rule mining: \textit{\$ java -jar rumis-1.0.jar -e=pos -l=data/sample.imdb.txt}. The \textit{horn-rules.txt} should be then stored in the same folder as the RUMIS jar file.

\textit{Nonmonotonic rule mining.} The following command for executing IMDB nonmonotonic rule mining with OPM ranking can be run: \textit{\$ java -jar rumis-1.0.jar -e=neg -p=horn-rules.txt -l=data/sample.imdb.txt -r=2}. One may just want to revise top 10 Horn rules with the \textit{-t} option: \textit{\$ java -jar rumis-1.0.jar -e=neg -p=horn-rules.txt -l=data/sample.imdb.txt -r=2 -t=10}. After these commands, \textit{revised-rules.txt} in the root folder is produced, which stores the computed nonmonotonic rules.

\textit{Automatic experiment.} The working folder can be built as a directory \textit{data/experiment/IMDB} that contains all of the following files. First, \textit{sample.imdb.txt} should be renamed to \textit{ideal.data.txt} in the directory. Second, the learning data of the ideal KG should be sampled to \textit{training.data.txt}. Third, one should generate \textit{horn-rules.txt} as an output of positive rule mining function applied to the learning data. If only some positive rules need to be revised, one can list them in \textit{selected.horn-rules.txt}. Finally, DLV binary file should be downloaded to the working directory and renamed as \textit{dlv.bin}.

The command that executes the experiment with OPM ranking and top 10 positive rules (with DLV) is: \textit{java -jar rumis-1.0.jar -e=exp -f=data/experiment/IMDB/ -r=2 -t=10 -d 1$>$experiment.txt 2$>$log}.

%\subsection{Usage Description}
%
%First of all, please download the repository\footnote{\url{https://github.com/htran010589/nonmonotonic-rule-mining}}, and then uncompress \textit{data/sample.imdb.txt.zip} to get \textit{sample.imdb.txt} file. In the next step, the repository folder should be located: \textit{\$ cd nonmonotonic-rule-mining}. Now the preparation is finished, and we are ready to describe the main features of RUMIS in details.
%
%\textbf{Generating Learning KG.} Please generate the learning data in the SPO format with the following command: \textit{\$ java -jar rumis-1.0.jar -e=new -l=[path to KG file] -o=[ratio] 1$>$[training KG file path]}.
%
%\textit{Example.} A learning KG of the IMDB sample dataset can be generated with predicate ratio being 80\%: \textit{\$ java -jar rumis-1.0.jar -e=new -l=data/sample.imdb.txt -o=0.8 1$>$training.sample.imdb.txt}. Then \textit{training.sample.imdb.txt} file is the learning KG that we want to generate.
%
%\textbf{Horn Rule Mining.} Please run the Horn rule mining with the following command: \textit{\$ java -jar rumis-1.0.jar -e=pos -l=[path to KG file]}. This will generate two files, \textit{horn-rules.txt} for the positive rules and \textit{horn-rules-stats.txt} with the presence of \textit{absolute support}. Both rules in two files are sorted by decreasing order of the support measure.
%
%\textit{Example.} Please run the following command for executing IMDB Horn rule mining: \textit{\$ java -jar rumis-1.0.jar -e=pos -l=data/sample.imdb.txt}. Two generated files horn-rules.txt and horn-rules-stats.txt can be seen in the same folder with RUMIS jar file.
%
%\textbf{Nonmonotonic Rule Mining.} Please run the nonmonotonic rule mining with the following command: \textit{\$ java -jar rumis-1.0.jar -e=neg -p=[path to positive rule file] -l=[path to KG file] -r=[ranking option] -t=[top Horn rules]}. Revised rules can be seen from generated result file \textit{revised-rules.txt} in the same folder with RUMIS jar file. Note that \textit{horn-rules.txt} is generated by using above Horn rule mining function, or by another software. However, RUMIS currently only supports Horn rules in format~\ref{form2}.
%
%The file \textit{revised-rules.txt} lists the revisions of Horn rules from \textit{horn-rules.txt} with the ranking option being specified in the command. Two main sections are presented in \textit{revised-rules.txt}, the first one describes exceptions ranked for each Horn rule and the second one lists final selected revisions. As regards the first section, Naive ranking subsection is always presented on top of the file, followed by [PM $|$ OPM] one if [PM $|$ OPM] is selected, resp.
%
%Every ranking subsection contains many parts separated by a blank line, each of them describes rules with exceptions. The first line of each part is a positive rule and its measure values with the format: \textit{$<$positive rule$>$ $<$Conv$>$ $<$Conf$>$}. \textit{Conv} and \textit{Conf} are abbreviations of conviction and confidence measures, resp.
%
%The rest of each above part is top 10 negated atoms for each Horn rule which are sorted according to the decreasing order of the positive-negative conviction (\textit{PosNegConv}) of the corresponding revision achieved by inserting the negated atom to the rule. If two revisions have the same positive-negative conviction, the one with higher conviction has a higher rank. The format for describing each negated atom with its measures is: \textit{$<$not exception$>$ $<$PosNegConv$>$ $<$Conv$>$}.
%
%The second section of the \textit{revised-rules.txt} lists chosen revisions for all the Horn rules. They are corresponding to the best exceptions of positive rules in the subsection of given ranking option.
%
%\textit{Example.} Please run the following command for executing IMDB nonmonotonic rule mining with OPM ranking: \textit{\$ java -jar rumis-1.0.jar -e=neg -p=horn-rules.txt -l=data/sample.imdb.txt -r=2}. One may just want to revise top 10 Horn rules with the \textit{-t} option: \textit{\$ java -jar rumis-1.0.jar -e=neg -p=horn-rules.txt -l=data/sample.imdb.txt -r=2 -t=10}.
%
%\textbf{Experiment.} First of all, please create the working folder indicated above, and then run the following command for the experiment: \textit{java -XX:-UseGCOverheadLimit -Xmx[max memory]G -jar rumis-1.0.jar -e=exp -f=[working folder] -r=[ranking] -t=[top positive rules] -d -s 1$>$experiment.txt 2$>$log} where \textit{-XX, -Xmx, -t, -d, -s} are optional. \textit{-XX} and \textit{-Xmx} are used when we want to allocate more memory for RUMIS, and, if the \textit{-t} is not used, all the rules in \textit{horn-rules.txt} are revised. Besides, \textit{-s} option should only be added to the command if we just want to care about revisions of some selected rules in \textit{selected.horn-rules.txt} file.
%
%The first two lines of the generated file \textit{experiment.txt} present the average conviction of selected Horn rules and their final revisions. These statistics are described in the Table 1 in the next chapter with different number of top Horn rules. Besides, the rest two parts of the file show predicates extended from the learning KG for positive and revised rules, resp. Format of every tab separated line in each part is \textit{$<$relation$>$ $<$inferred facts$>$ $<$good facts$>$ $<$other facts$>$} where \textit{inferred facts} means total number of predicted triples over the relation. Besides, \textit{good} and \textit{other facts} indicate quantity of \textit{inferred facts} that are in and not in the ideal KG, resp.
%
%The command outputs a file \textit{encode.txt} and a DLV subdirectory in the working folder. The former is a tab separated file that maps from entities and predicates of ideal KG to their encoded IDs. The latter provides some files as follows. First, \textit{training.data.kg} is the DLV format version of \textit{training.data.txt}. Second, files \textit{chosen.rules.[naive $|$ pm $|$ opm].txt.[pos $|$ neg].[number of top rules]} list chosen rules in encoded DLV format, i.e, DLV rule format s.t. predicated and exceptions are encoded. Third, files \textit{extension.[naive $|$ pm $|$ opm].txt.[pos $|$ neg].[number of top rules]} describe KGs extended from learning data in DLV format and corresponding rules. The terms \textit{pos, neg} correspond to positive and revised rulesets which are exploited to extend KG. Finally, For each of these, there are \textit{decode, needcheck, good, conflict} extension files which present KG in SPO format, facts not in the ideal KG, facts in the ideal KG and conflicts, resp.
%
%\textit{Example.} The working folder can be built as a directory \textit{data/experiment/IMDB} that contains all of the following files. First, please rename \textit{sample.imdb.txt} file to \textit{ideal.data.txt} in the directory. Second, please sample the learning data of the ideal KG file and get \textit{training.data.txt}. Third, one should generate \textit{horn-rules.txt} as an output of positive rule mining function applied to the learning data. If only some positive rules need to be revised, we can list them in \textit{selected.horn-rules.txt}. Finally, DLV binary file should be downloaded to the directory.
%
%The command that executes experiment with OPM ranking and top 10 positive rules (without DLV) is: \textit{java -jar rumis-1.0.jar -e=exp -f=data/experiment/IMDB/ -r=2 -t=10 1$>$experiment.txt 2$>$log}. If we want to expand memory, enable DLV and selected rule option, the following command can be used. \textit{java -XX:-UseGCOverheadLimit -Xmx300G -jar rumis-1.0.jar -e=exp -f=data/experiment/IMDB/ -r=2 -t=10 -d -s 1$>$experiment.txt 2$>$log}.